{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrk022/ASR_model/blob/main/ASR_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDM3IHvSVUYy"
      },
      "outputs": [],
      "source": [
        "# 1. Install system dependencies for audio\n",
        "!apt-get update && apt-get install -y libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "\n",
        "# 2. Install all Python packages\n",
        "!pip install torchaudio transformers speechbrain datasets pyaudio soundfile\n",
        "\n",
        "# 3. Additional dependencies for Hugging Face datasets\n",
        "!pip install librosa pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bXlDYpQedKEl"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y libasound2-dev portaudio19-dev\n",
        "!pip install git+https://github.com/Uberi/speech_recognition.git\n",
        "!pip install pyaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"superb\", \"asr\", split=\"train[:100]\", trust_remote_code=True)\n",
        "\n",
        "print(f\"Loaded {len(dataset)} samples\")\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "LHgxW2Q7r4Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "def preprocess_common_voice(batch):\n",
        "    return {\n",
        "        \"audio\": np.array(batch[\"audio\"][\"array\"], dtype=np.float32),\n",
        "        \"text\": batch[\"text\"],\n",
        "        \"accent\": \"us\"  # Default\n",
        "    }\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_common_voice = dataset.map(preprocess_common_voice)\n",
        "\n",
        "# Create synthetic data with varied accents\n",
        "synthetic_samples = Dataset.from_dict({\n",
        "    \"audio\": [np.random.rand(16000).astype(np.float32) for _ in range(10)],\n",
        "    \"text\": [f\"Sample {i}\" for i in range(10)],\n",
        "    \"accent\": [\n",
        "        *[\"us\"]*2, *[\"uk\"]*2, *[\"indian\"],\n",
        "        *[\"australian\"], *[\"canadian\"],\n",
        "        *[\"african\"]*3\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Merge datasets\n",
        "final_dataset = concatenate_datasets([processed_common_voice, synthetic_samples])\n",
        "\n",
        "# Show accent distribution\n",
        "print(f\"\\nFinal dataset contains {len(final_dataset)} samples:\")\n",
        "for accent in set(final_dataset[\"accent\"]):\n",
        "    print(f\"- {accent}: {final_dataset['accent'].count(accent)}\")\n"
      ],
      "metadata": {
        "id": "8DVQK_Omr4HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline\n",
        "\n",
        "class AccentAdaptiveASR:\n",
        "    def __init__(self):\n",
        "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "        self.model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "        self.corrector = pipeline(\"text2text-generation\", model=\"grammarly/coedit-large\")\n",
        "        self.accent_embedding = torch.nn.Embedding(5, 1024)  # 5 accents\n",
        "\n",
        "    def transcribe(self, audio_data, accent_id=0):\n",
        "        if isinstance(audio_data, np.ndarray):\n",
        "            audio_data = torch.FloatTensor(audio_data) / 32768.0\n",
        "\n",
        "        inputs = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states[-1]\n",
        "            accent_features = self.accent_embedding(torch.tensor([accent_id]))\n",
        "            adapted = hidden_states + accent_features.unsqueeze(1)\n",
        "            logits = self.model.lm_head(adapted)\n",
        "            predicted_ids = torch.argmax(logits, dim=-1)\n",
        "            raw_text = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "        corrected = self.corrector(raw_text)[0]['generated_text']\n",
        "        return {\"raw\": raw_text, \"corrected\": corrected}\n"
      ],
      "metadata": {
        "id": "EMr3KAG6r4ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def check_microphone():\n",
        "    recognizer = sr.Recognizer()\n",
        "    try:\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Testing microphone...\")\n",
        "            recognizer.adjust_for_ambient_noise(source)\n",
        "            return True\n",
        "    except OSError:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "MVi2V00yr4Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "\n",
        "def live_accent_recognition():\n",
        "    if not check_microphone():\n",
        "        print(\"\\nNo microphone detected. Using test audio 'sample.wav'...\")\n",
        "        try:\n",
        "            waveform, _ = torchaudio.load(\"sample.wav\")\n",
        "            audio_data = waveform.numpy()[0]\n",
        "            asr_system = AccentAdaptiveASR()\n",
        "            result = asr_system.transcribe(audio_data, accent_id=2)\n",
        "\n",
        "            print(\"\\nResults from sample.wav:\")\n",
        "            print(f\"Raw: {result['raw']}\")\n",
        "            print(f\"Corrected: {result['corrected']}\")\n",
        "            return\n",
        "        except FileNotFoundError:\n",
        "            print(\"No sample.wav file found.\")\n",
        "            return\n",
        "\n",
        "    recognizer = sr.Recognizer()\n",
        "    asr_system = AccentAdaptiveASR()\n",
        "\n",
        "    print(\"\\n=== Real-Time Accent-Adaptive ASR ===\")\n",
        "    print(\"Accent Options: 0-US, 1-UK, 2-Indian, 3-Australian, 4-African\")\n",
        "\n",
        "    try:\n",
        "        accent_id = int(input(\"Select accent (0â€“4): \"))\n",
        "    except:\n",
        "        accent_id = 0\n",
        "\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening for 5 seconds...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source, timeout=5)\n",
        "        audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16)\n",
        "\n",
        "    result = asr_system.transcribe(audio_data, accent_id)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Raw Transcription: {result['raw']}\")\n",
        "    if result['raw'].lower() != result['corrected'].lower():\n",
        "        print(f\"Corrected Version: {result['corrected']}\")\n",
        "    else:\n",
        "        print(\"(No corrections needed)\")\n"
      ],
      "metadata": {
        "id": "xt6_u2WOr39q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "live_accent_recognition()\n"
      ],
      "metadata": {
        "id": "JDZKSZGWr36v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPULLhaxtIfZsI+Pl4QpYW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}